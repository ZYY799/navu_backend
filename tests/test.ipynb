{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from IPython.display import Video, display, HTML, Image as IPImage\n",
    "import tempfile\n",
    "from ultralytics import YOLO\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "b6079d7359121ef2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def setup_chinese_font():\n",
    "    system = platform.system()\n",
    "\n",
    "    chinese_fonts = []\n",
    "\n",
    "    if system == 'Darwin':  # macOS\n",
    "        chinese_fonts = [\n",
    "            '/System/Library/Fonts/PingFang.ttc',\n",
    "            '/System/Library/Fonts/Hiragino Sans GB.ttc',\n",
    "        ]\n",
    "    elif system == 'Windows':\n",
    "        chinese_fonts = [\n",
    "            'C:/Windows/Fonts/msyh.ttc',  # å¾®è½¯é›…é»‘\n",
    "            'C:/Windows/Fonts/simhei.ttf',  # é»‘ä½“\n",
    "            'C:/Windows/Fonts/simsun.ttc',  # å®‹ä½“\n",
    "        ]\n",
    "    else:  # Linux\n",
    "        chinese_fonts = [\n",
    "            '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',\n",
    "            '/usr/share/fonts/truetype/arphic/uming.ttc',\n",
    "        ]\n",
    "\n",
    "    font_path = None\n",
    "    for font in chinese_fonts:\n",
    "        if Path(font).exists():\n",
    "            font_path = font\n",
    "            break\n",
    "\n",
    "    if font_path:\n",
    "        try:\n",
    "            font_prop = font_manager.FontProperties(fname=font_path)\n",
    "            plt.rcParams['font.family'] = font_prop.get_name()\n",
    "            print(f\"ä¸­æ–‡å­—ä½“åŠ è½½æˆåŠŸ: {font_path}\")\n",
    "            return font_path\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡æ³¨\")\n",
    "    return None\n",
    "\n",
    "CHINESE_FONT_PATH = setup_chinese_font()\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ],
   "id": "ffb07d15af5c3fd6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_chinese_font_for_cv2(font_path: str = None, size: int = 20):\n",
    "\n",
    "    from PIL import ImageFont\n",
    "\n",
    "    if font_path is None:\n",
    "        font_path = CHINESE_FONT_PATH\n",
    "\n",
    "    if font_path and Path(font_path).exists():\n",
    "        try:\n",
    "            return ImageFont.truetype(font_path, size)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return None"
   ],
   "id": "3870bce066e03515"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def put_chinese_text(img, text, position, font_size=20, color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    åœ¨OpenCVå›¾åƒä¸Šç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬ï¼ˆå¸¦é»‘è‰²æè¾¹ï¼Œæ— èƒŒæ™¯ï¼‰\n",
    "\n",
    "    Args:\n",
    "        img: OpenCVå›¾åƒ\n",
    "        text: æ–‡æœ¬å†…å®¹\n",
    "        position: (x, y) ä½ç½®\n",
    "        font_size: å­—ä½“å¤§å°\n",
    "        color: æ–‡å­—é¢œè‰² (B, G, R)\n",
    "    \"\"\"\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    font = get_chinese_font_for_cv2(size=font_size)\n",
    "    if font is None:\n",
    "        x, y = position\n",
    "        cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   font_size/30, (0, 0, 0), 3, cv2.LINE_AA)  # é»‘è‰²æè¾¹\n",
    "        cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   font_size/30, color, 2, cv2.LINE_AA)  # å½©è‰²æ–‡å­—\n",
    "        return img\n",
    "\n",
    "    x, y = position\n",
    "\n",
    "    # PILé¢œè‰²è½¬æ¢ (BGR -> RGB)\n",
    "    pil_color = (color[2], color[1], color[0])\n",
    "    outline_color = (0, 0, 0)  # é»‘è‰²æè¾¹\n",
    "\n",
    "    # ç»˜åˆ¶æè¾¹ï¼ˆ8ä¸ªæ–¹å‘ï¼‰\n",
    "    outline_width = max(2, font_size // 10)\n",
    "    for dx in [-outline_width, 0, outline_width]:\n",
    "        for dy in [-outline_width, 0, outline_width]:\n",
    "            if dx == 0 and dy == 0:\n",
    "                continue\n",
    "            draw.text((x + dx, y + dy), text, font=font, fill=outline_color)\n",
    "\n",
    "    # ç»˜åˆ¶ä¸»æ–‡å­—ï¼ˆå½©è‰²ï¼‰\n",
    "    draw.text((x, y), text, font=font, fill=pil_color)\n",
    "\n",
    "    # è½¬å›OpenCV\n",
    "    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)"
   ],
   "id": "f97b61c9be6d41bc",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class StreetObstacleDetector:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_path: str = \"yolo11x.pt\",\n",
    "                 confidence: float = 0.3,\n",
    "                 label_font_size: int = 18,\n",
    "                 box_thickness: int = 2):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ£€æµ‹å™¨\n",
    "\n",
    "        Args:\n",
    "            model_path: YOLOæ¨¡å‹è·¯å¾„\n",
    "            confidence: ç½®ä¿¡åº¦é˜ˆå€¼\n",
    "            label_font_size: æ ‡ç­¾å­—ä½“å¤§å°\n",
    "            box_thickness: è¾¹æ¡†ç²—ç»†\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.confidence = confidence\n",
    "        self.label_font_size = label_font_size\n",
    "        self.box_thickness = box_thickness\n",
    "\n",
    "        # éšœç¢ç‰©ç±»åˆ«æ˜ å°„ (COCOæ•°æ®é›†)\n",
    "        self.obstacle_classes = {\n",
    "            0: (\"è¡Œäºº\", \"Person\"),\n",
    "            1: (\"è‡ªè¡Œè½¦\", \"Bicycle\"),\n",
    "            2: (\"æ±½è½¦\", \"Car\"),\n",
    "            3: (\"æ‘©æ‰˜è½¦\", \"Motorcycle\"),\n",
    "            5: (\"å…¬äº¤è½¦\", \"Bus\"),\n",
    "            7: (\"å¡è½¦\", \"Truck\"),\n",
    "            9: (\"çº¢ç»¿ç¯\", \"Traffic Light\"),\n",
    "            11: (\"åœæ­¢æ ‡å¿—\", \"Stop Sign\"),\n",
    "            12: (\"åœè½¦è®¡æ—¶å™¨\", \"Parking Meter\"),\n",
    "        }\n",
    "\n",
    "        # å±é™©ç­‰çº§é¢œè‰² (BGRæ ¼å¼)\n",
    "        self.colors = {\n",
    "            \"high\": (0, 0, 255),      # çº¢è‰²\n",
    "            \"medium\": (0, 165, 255),   # æ©™è‰²\n",
    "            \"low\": (0, 255, 0),        # ç»¿è‰²\n",
    "            \"info\": (255, 255, 0)      # é’è‰²\n",
    "        }\n",
    "\n",
    "        print(f\"YOLOæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}\")\n",
    "\n",
    "    def _calculate_distance(self, bbox: List[float], img_height: int, img_width: int) -> float:\n",
    "\n",
    "        x1, y1, x2, y2 = bbox\n",
    "\n",
    "        # è®¡ç®—æ¡†çš„é¢ç§¯å æ¯”\n",
    "        box_area = (x2 - x1) * (y2 - y1)\n",
    "        img_area = img_height * img_width\n",
    "        area_ratio = box_area / img_area\n",
    "\n",
    "        # è®¡ç®—æ¡†åº•éƒ¨ä½ç½®ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰\n",
    "        bottom_ratio = y2 / img_height\n",
    "\n",
    "        # è®¡ç®—æ¡†é«˜åº¦å æ¯”\n",
    "        height_ratio = (y2 - y1) / img_height\n",
    "\n",
    "        # ç»¼åˆä¼°ç®—è·ç¦»\n",
    "        if area_ratio > 0.3:\n",
    "            distance = 1.0\n",
    "        elif area_ratio > 0.2:\n",
    "            distance = 2.0\n",
    "        elif area_ratio > 0.1:\n",
    "            distance = 3.5\n",
    "        elif area_ratio > 0.05:\n",
    "            distance = 6.0\n",
    "        elif area_ratio > 0.02:\n",
    "            distance = 10.0\n",
    "        else:\n",
    "            if bottom_ratio > 0.9:\n",
    "                distance = 8.0\n",
    "            elif bottom_ratio > 0.7:\n",
    "                distance = 15.0\n",
    "            elif bottom_ratio > 0.5:\n",
    "                distance = 25.0\n",
    "            else:\n",
    "                distance = 40.0\n",
    "\n",
    "        # æ ¹æ®é«˜åº¦å æ¯”å¾®è°ƒ\n",
    "        if height_ratio > 0.6:\n",
    "            distance *= 0.6\n",
    "        elif height_ratio > 0.4:\n",
    "            distance *= 0.8\n",
    "\n",
    "        return round(distance, 1)\n",
    "\n",
    "    def _get_direction(self, bbox: List[float], img_width: int) -> Tuple[str, str]:\n",
    "\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        center_x = (x1 + x2) / 2\n",
    "\n",
    "        if center_x < img_width * 0.33:\n",
    "            return \"å·¦å‰æ–¹\", \"Left\"\n",
    "        elif center_x > img_width * 0.67:\n",
    "            return \"å³å‰æ–¹\", \"Right\"\n",
    "        else:\n",
    "            return \"æ­£å‰æ–¹\", \"Front\"\n",
    "\n",
    "    def _get_danger_level(self, class_name: str, distance: float, area_ratio: float) -> str:\n",
    "\n",
    "        if class_name == \"çº¢ç»¿ç¯\":\n",
    "            return \"info\"\n",
    "\n",
    "        # è¡Œäºº/è‡ªè¡Œè½¦/æ‘©æ‰˜è½¦\n",
    "        if class_name in [\"è¡Œäºº\", \"è‡ªè¡Œè½¦\", \"æ‘©æ‰˜è½¦\"]:\n",
    "            if distance < 3:\n",
    "                return \"high\"\n",
    "            elif distance < 6:\n",
    "                return \"medium\"\n",
    "            else:\n",
    "                return \"low\"\n",
    "\n",
    "        # æ±½è½¦/å¡è½¦/å…¬äº¤è½¦\n",
    "        elif class_name in [\"æ±½è½¦\", \"å¡è½¦\", \"å…¬äº¤è½¦\"]:\n",
    "            if distance < 2:\n",
    "                return \"high\"\n",
    "            elif distance < 5:\n",
    "                return \"medium\"\n",
    "            else:\n",
    "                return \"low\"\n",
    "\n",
    "        # å…¶ä»–éšœç¢ç‰©\n",
    "        else:\n",
    "            if distance < 4:\n",
    "                return \"medium\"\n",
    "            else:\n",
    "                return \"low\"\n",
    "\n",
    "    def _find_label_position(self, bbox: List[float], occupied_regions: List,\n",
    "                            img_height: int, img_width: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        æ™ºèƒ½æŸ¥æ‰¾æ ‡ç­¾ä½ç½®ï¼Œé¿å…é‡å \n",
    "\n",
    "        Args:\n",
    "            bbox: å½“å‰æ£€æµ‹æ¡†\n",
    "            occupied_regions: å·²è¢«å ç”¨çš„åŒºåŸŸåˆ—è¡¨ [(x, y, w, h), ...]\n",
    "            img_height: å›¾åƒé«˜åº¦\n",
    "            img_width: å›¾åƒå®½åº¦\n",
    "\n",
    "        Returns:\n",
    "            (x, y) æ ‡ç­¾å·¦ä¸Šè§’ä½ç½®\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        label_width = 200\n",
    "        label_height = 30\n",
    "\n",
    "        candidates = [\n",
    "            (int(x1), max(0, int(y1) - label_height - 5)),  # ä¸Šæ–¹\n",
    "            (int(x1), int(y2) + 5),                          # ä¸‹æ–¹\n",
    "            (max(0, int(x1) - label_width - 5), int(y1)),   # å·¦ä¾§\n",
    "            (int(x2) + 5, int(y1)),                          # å³ä¾§\n",
    "            (int(x1) + 5, int(y1) + 5),                      # æ¡†å†…å·¦ä¸Šè§’\n",
    "        ]\n",
    "\n",
    "        for x, y in candidates:\n",
    "            # è¾¹ç•Œæ£€æŸ¥\n",
    "            if x < 0 or y < 0 or x + label_width > img_width or y + label_height > img_height:\n",
    "                continue\n",
    "\n",
    "            overlap = False\n",
    "            for ox, oy, ow, oh in occupied_regions:\n",
    "                if not (x + label_width < ox or x > ox + ow or\n",
    "                       y + label_height < oy or y > oy + oh):\n",
    "                    overlap = True\n",
    "                    break\n",
    "\n",
    "            if not overlap:\n",
    "                return (x, y)\n",
    "\n",
    "        return (int(x1), max(0, int(y1) - label_height - 5))\n",
    "\n",
    "    def detect_frame(self, frame: np.ndarray, use_chinese: bool = True) -> Tuple[np.ndarray, List[Dict]]:\n",
    "        \"\"\"\n",
    "        æ£€æµ‹å•å¸§å›¾åƒï¼ˆä¼˜åŒ–æ ‡æ³¨å¸ƒå±€ï¼‰\n",
    "\n",
    "        Args:\n",
    "            frame: è¾“å…¥å›¾åƒ\n",
    "            use_chinese: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æ ‡æ³¨\n",
    "\n",
    "        Returns:\n",
    "            (æ ‡æ³¨åçš„å›¾åƒ, æ£€æµ‹ç»“æœåˆ—è¡¨)\n",
    "        \"\"\"\n",
    "        img_height, img_width = frame.shape[:2]\n",
    "\n",
    "        results = self.model(frame, conf=self.confidence, verbose=False)[0]\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        detections = []\n",
    "        occupied_regions = []\n",
    "\n",
    "        if hasattr(results, 'boxes') and len(results.boxes) > 0:\n",
    "            for det in results.boxes.data:\n",
    "                x1, y1, x2, y2, conf, cls = det.cpu().numpy()\n",
    "                class_id = int(cls)\n",
    "\n",
    "                if class_id not in self.obstacle_classes:\n",
    "                    continue\n",
    "\n",
    "                class_name_cn, class_name_en = self.obstacle_classes[class_id]\n",
    "                confidence = float(conf)\n",
    "                bbox = [float(x1), float(y1), float(x2), float(y2)]\n",
    "\n",
    "                box_area = (x2 - x1) * (y2 - y1)\n",
    "                img_area = img_height * img_width\n",
    "                area_ratio = box_area / img_area\n",
    "\n",
    "                distance = self._calculate_distance(bbox, img_height, img_width)\n",
    "                direction_cn, direction_en = self._get_direction(bbox, img_width)\n",
    "                danger_level = self._get_danger_level(class_name_cn, distance, area_ratio)\n",
    "\n",
    "                detections.append({\n",
    "                    \"class_cn\": class_name_cn,\n",
    "                    \"class_en\": class_name_en,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"bbox\": bbox,\n",
    "                    \"distance\": distance,\n",
    "                    \"direction_cn\": direction_cn,\n",
    "                    \"direction_en\": direction_en,\n",
    "                    \"danger_level\": danger_level,\n",
    "                    \"area_ratio\": area_ratio\n",
    "                })\n",
    "\n",
    "                color = self.colors[danger_level]\n",
    "                x1_int, y1_int, x2_int, y2_int = map(int, bbox)\n",
    "\n",
    "                cv2.rectangle(annotated_frame, (x1_int, y1_int), (x2_int, y2_int),\n",
    "                             color, self.box_thickness)\n",
    "\n",
    "                label_x, label_y = self._find_label_position(\n",
    "                    bbox, occupied_regions, img_height, img_width\n",
    "                )\n",
    "\n",
    "                if use_chinese and CHINESE_FONT_PATH:\n",
    "\n",
    "                    label = f\"{class_name_cn} {confidence:.2f} {direction_cn} {distance}m\"\n",
    "\n",
    "                    annotated_frame = put_chinese_text(\n",
    "                        annotated_frame, label, (label_x, label_y),\n",
    "                        font_size=self.label_font_size,\n",
    "                        color=color\n",
    "                    )\n",
    "\n",
    "                    occupied_regions.append((label_x, label_y, 200, 30))\n",
    "\n",
    "                else:\n",
    "                    label = f\"{class_name_en} {confidence:.2f} {direction_en} {distance}m\"\n",
    "\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    font_scale = 0.5\n",
    "                    thickness = 2\n",
    "\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "\n",
    "                    cv2.putText(annotated_frame, label,\n",
    "                               (label_x, label_y + text_h),\n",
    "                               font, font_scale, (0, 0, 0), thickness + 2, cv2.LINE_AA)\n",
    "\n",
    "                    cv2.putText(annotated_frame, label,\n",
    "                               (label_x, label_y + text_h),\n",
    "                               font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "                    occupied_regions.append((label_x, label_y, text_w + 10, text_h + 10))\n",
    "\n",
    "        return annotated_frame, detections"
   ],
   "id": "3c153a295b29c87e",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_distance_calculation():\n",
    "\n",
    "    detector = StreetObstacleDetector()\n",
    "\n",
    "    img_height, img_width = 960, 544\n",
    "\n",
    "    test_cases = [\n",
    "        ([100, 500, 500, 900], \"å¤§æ¡†ï¼Œå æ®ç”»é¢å¾ˆå¤§éƒ¨åˆ†ï¼Œé è¿‘åº•éƒ¨\"),\n",
    "        ([150, 600, 400, 850], \"ä¸­ç­‰æ¡†ï¼Œé è¿‘åº•éƒ¨\"),\n",
    "        ([200, 400, 350, 600], \"ä¸­ç­‰æ¡†ï¼Œä¸­é—´ä½ç½®\"),\n",
    "        ([250, 200, 350, 350], \"å°æ¡†ï¼Œé ä¸Šä½ç½®\"),\n",
    "        ([100, 100, 200, 200], \"å°æ¡†ï¼Œé¡¶éƒ¨ä½ç½®\"),\n",
    "    ]\n",
    "\n",
    "    print(\"è·ç¦»ä¼°ç®—æµ‹è¯•ï¼š\\n\")\n",
    "    for bbox, desc in test_cases:\n",
    "        distance = detector._calculate_distance(bbox, img_height, img_width)\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        area = (x2-x1) * (y2-y1)\n",
    "        area_ratio = area / (img_height * img_width)\n",
    "        print(f\"{desc}\")\n",
    "        print(f\"  æ¡†å¤§å°: {x2-x1}x{y2-y1} (é¢ç§¯å æ¯”: {area_ratio*100:.1f}%)\")\n",
    "        print(f\"  ä¼°ç®—è·ç¦»: {distance}m\\n\")"
   ],
   "id": "a798cceb0dfef86d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_random_frame(video_path: str,\n",
    "                      model_path: str = \"yolo11x.pt\",\n",
    "                      confidence: float = 0.3,\n",
    "                      use_chinese: bool = True):\n",
    "    \"\"\"\n",
    "    éšæœºæµ‹è¯•ä¸€å¸§ï¼Œå¿«é€ŸæŸ¥çœ‹æ•ˆæœ\n",
    "\n",
    "    Args:\n",
    "        video_path: è§†é¢‘è·¯å¾„\n",
    "        model_path: YOLOæ¨¡å‹è·¯å¾„\n",
    "        confidence: ç½®ä¿¡åº¦é˜ˆå€¼\n",
    "        use_chinese: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æ ‡æ³¨\n",
    "    \"\"\"\n",
    "    print(\"å•å¸§æµ‹è¯•æ¨¡å¼\")\n",
    "\n",
    "    detector = StreetObstacleDetector(model_path, confidence)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    random_frame_idx = np.random.randint(0, total_frames)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_idx)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        raise ValueError(\"æ— æ³•è¯»å–å¸§\")\n",
    "\n",
    "    print(f\"éšæœºå¸§: {random_frame_idx}/{total_frames}\")\n",
    "\n",
    "    annotated_frame, detections = detector.detect_frame(frame, use_chinese)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    ax1.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    ax1.set_title('åŸå§‹å›¾åƒ', fontsize=16)\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB))\n",
    "    ax2.set_title(f'æ£€æµ‹ç»“æœ (æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡)', fontsize=16)\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if detections:\n",
    "        print(f\"\\næ£€æµ‹è¯¦æƒ…:\")\n",
    "        for i, det in enumerate(detections, 1):\n",
    "            print(f\"  {i}. {det['class_cn']} ({det['class_en']})\")\n",
    "            print(f\"     ä½ç½®: {det['direction_cn']} ({det['direction_en']})\")\n",
    "            print(f\"     è·ç¦»: {det['distance']}m\")\n",
    "            print(f\"     ç½®ä¿¡åº¦: {det['confidence']:.2f}\")\n",
    "            print(f\"     å±é™©ç­‰çº§: {det['danger_level']}\")\n",
    "    else:\n",
    "        print(\"æœªæ£€æµ‹åˆ°éšœç¢ç‰©\")\n",
    "\n",
    "    return annotated_frame, detections"
   ],
   "id": "2a5921b7228f22c5",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_video_with_progress(input_path: str,\n",
    "                                output_path: str = None,\n",
    "                                model_path: str = \"yolo11x.pt\",\n",
    "                                confidence: float = 0.3,\n",
    "                                use_chinese: bool = True,\n",
    "                                show_info: bool = True,\n",
    "                                save_detection_log: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    å¤„ç†è§†é¢‘å¹¶ç”Ÿæˆæ£€æµ‹ç»“æœ (å¸¦è¿›åº¦æ¡)\n",
    "\n",
    "    Args:\n",
    "        input_path: è¾“å…¥è§†é¢‘è·¯å¾„\n",
    "        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„\n",
    "        model_path: YOLOæ¨¡å‹è·¯å¾„\n",
    "        confidence: ç½®ä¿¡åº¦é˜ˆå€¼\n",
    "        use_chinese: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æ ‡æ³¨\n",
    "        show_info: æ˜¯å¦åœ¨è§†é¢‘ä¸Šæ˜¾ç¤ºå¸§ä¿¡æ¯\n",
    "        save_detection_log: æ˜¯å¦ä¿å­˜æ£€æµ‹æ—¥å¿—\n",
    "\n",
    "    Returns:\n",
    "        è¾“å‡ºè§†é¢‘è·¯å¾„\n",
    "    \"\"\"\n",
    "    print(\"å®Œæ•´è§†é¢‘å¤„ç†æ¨¡å¼\")\n",
    "\n",
    "    if not Path(input_path).exists():\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶: {input_path}\")\n",
    "\n",
    "    if output_path is None:\n",
    "        input_path_obj = Path(input_path)\n",
    "        output_path = str(input_path_obj.parent / f\"{input_path_obj.stem}_detected{input_path_obj.suffix}\")\n",
    "\n",
    "    detector = StreetObstacleDetector(model_path, confidence)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"æ— æ³•æ‰“å¼€è§†é¢‘: {input_path}\")\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"è§†é¢‘ä¿¡æ¯:\")\n",
    "    print(f\"   åˆ†è¾¨ç‡: {width}x{height}\")\n",
    "    print(f\"   å¸§ç‡: {fps} fps\")\n",
    "    print(f\"   æ€»å¸§æ•°: {total_frames}\")\n",
    "    print(f\"   æ—¶é•¿: {total_frames/fps:.1f}ç§’\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    detection_log = []\n",
    "\n",
    "    print(\"\\nå¼€å§‹å¤„ç†...\")\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    pbar = tqdm(total=total_frames, desc=\"å¤„ç†è¿›åº¦\", unit=\"å¸§\", ncols=100)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        annotated_frame, detections = detector.detect_frame(frame, use_chinese)\n",
    "\n",
    "        if show_info:\n",
    "            info_text = f\"Frame: {frame_count}/{total_frames} | Objects: {len(detections)}\"\n",
    "            cv2.putText(annotated_frame, info_text,\n",
    "                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                       0.7, (0, 255, 255), 2)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        if save_detection_log and detections:\n",
    "            detection_log.append({\n",
    "                \"frame\": frame_count,\n",
    "                \"timestamp\": frame_count / fps,\n",
    "                \"detections\": detections\n",
    "            })\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"æ£€æµ‹å¯¹è±¡\": len(detections)})\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"\\nå¤„ç†å®Œæˆ!\")\n",
    "    print(f\"è¾“å‡ºæ–‡ä»¶: {output_path}\")\n",
    "\n",
    "    if save_detection_log and detection_log:\n",
    "        log_path = Path(output_path).with_suffix('.txt')\n",
    "        with open(log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"è¡—æ™¯è§†é¢‘éšœç¢ç‰©æ£€æµ‹æ—¥å¿—\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "            for log in detection_log:\n",
    "                f.write(f\"å¸§ {log['frame']} (æ—¶é—´: {log['timestamp']:.2f}s)\\n\")\n",
    "                for det in log['detections']:\n",
    "                    f.write(f\"  - {det['class_cn']}: {det['direction_cn']} {det['distance']}m \"\n",
    "                           f\"(ç½®ä¿¡åº¦: {det['confidence']:.2f}, å±é™©ç­‰çº§: {det['danger_level']})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        print(f\"æ£€æµ‹æ—¥å¿—: {log_path}\")\n",
    "\n",
    "    return output_path\n"
   ],
   "id": "92f9cb9deacda28d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "INPUT_VIDEO = \"/Users/zyy/Downloads/e2eee24c8412776cd1517947ff225846.mp4\"\n",
    "MODEL_PATH = \"yolo11x.pt\"\n",
    "CONFIDENCE = 0.35\n",
    "\n",
    "# å•å¸§æµ‹è¯•\n",
    "test_frame, test_detections = test_random_frame(\n",
    "    INPUT_VIDEO,\n",
    "    model_path=MODEL_PATH,\n",
    "    confidence=CONFIDENCE,\n",
    "    use_chinese=True\n",
    ")"
   ],
   "id": "2eba43cfe21d081e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_video = process_video_with_progress(\n",
    "    input_path=INPUT_VIDEO,\n",
    "    model_path=MODEL_PATH,\n",
    "    confidence=CONFIDENCE,\n",
    "    use_chinese=True,\n",
    "    show_info=True,\n",
    "    save_detection_log=True\n",
    ")"
   ],
   "id": "dc21d73b6def5b5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4680/4680 [18:19<00:00,  4.26å¸§/s, æ£€æµ‹å¯¹è±¡=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å¤„ç†å®Œæˆ!\n",
      "ğŸ“ è¾“å‡ºæ–‡ä»¶: /Users/zyy/Downloads/e2eee24c8412776cd1517947ff225846_detected.mp4\n",
      "ğŸ“„ æ£€æµ‹æ—¥å¿—: /Users/zyy/Downloads/e2eee24c8412776cd1517947ff225846_detected.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4ae713981af61541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff85c712fd47b879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e22df4089a1c2c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8bf74bb15d4944a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a4cf92b483c9b442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6844f0f876c76193",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aa7255f71d1dfb61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "518054d8a0e06119",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cedd385cbbcb5597",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ab0b7774fd0d375e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
